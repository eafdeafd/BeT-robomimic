
============= Initialized Observation Utils with Obs Spec =============

using obs modality: low_dim with keys: ['object', 'robot0_gripper_qpos', 'robot0_eef_pos', 'robot0_eef_quat']
using obs modality: rgb with keys: []
using obs modality: depth with keys: []
using obs modality: scan with keys: []
/home/andrew/Desktop/robo_sims/robomimic/robomimic/scripts

============= Loaded Environment Metadata =============
obs key object with shape (10,)
obs key robot0_eef_pos with shape (3,)
obs key robot0_eef_quat with shape (4,)
obs key robot0_gripper_qpos with shape (2,)
Created environment with name Lift
Action size is 7
Lift
{
    "camera_depths": false,
    "camera_heights": 84,
    "camera_widths": 84,
    "control_freq": 20,
    "controller_configs": {
        "control_delta": true,
        "damping": 1,
        "damping_limits": [
            0,
            10
        ],
        "impedance_mode": "fixed",
        "input_max": 1,
        "input_min": -1,
        "interpolation": null,
        "kp": 150,
        "kp_limits": [
            0,
            300
        ],
        "orientation_limits": null,
        "output_max": [
            0.05,
            0.05,
            0.05,
            0.5,
            0.5,
            0.5
        ],
        "output_min": [
            -0.05,
            -0.05,
            -0.05,
            -0.5,
            -0.5,
            -0.5
        ],
        "position_limits": null,
        "ramp_ratio": 0.2,
        "type": "OSC_POSE",
        "uncouple_pos_ori": true
    },
    "has_offscreen_renderer": true,
    "has_renderer": false,
    "ignore_done": true,
    "render_gpu_device_id": 0,
    "reward_shaping": false,
    "robots": [
        "Panda"
    ],
    "use_camera_obs": false,
    "use_object_obs": true
}


============= Model Summary =============
BET (
  ModuleDict(
    (mlp): MLP(
        input_dim=7
        output_dim=7
        layer_dims=[512, 512]
        layer_func=Linear
        dropout=None
        act=Mish
        output_act=Tanh
    )
    (policy): MinGPT(
      (model): GPT(
        (tok_emb): Linear(in_features=26, out_features=72, bias=True)
        (drop): Dropout(p=0.1, inplace=False)
        (blocks): Sequential(
          (0): Block(
            (ln1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
            (ln2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
            (attn): CausalSelfAttention(
              (key): Linear(in_features=72, out_features=72, bias=True)
              (query): Linear(in_features=72, out_features=72, bias=True)
              (value): Linear(in_features=72, out_features=72, bias=True)
              (attn_drop): Dropout(p=0.1, inplace=False)
              (resid_drop): Dropout(p=0.1, inplace=False)
              (proj): Linear(in_features=72, out_features=72, bias=True)
            )
            (mlp): Sequential(
              (0): Linear(in_features=72, out_features=288, bias=True)
              (1): GELU()
              (2): Linear(in_features=288, out_features=72, bias=True)
              (3): Dropout(p=0.1, inplace=False)
            )
          )
          (1): Block(
            (ln1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
            (ln2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
            (attn): CausalSelfAttention(
              (key): Linear(in_features=72, out_features=72, bias=True)
              (query): Linear(in_features=72, out_features=72, bias=True)
              (value): Linear(in_features=72, out_features=72, bias=True)
              (attn_drop): Dropout(p=0.1, inplace=False)
              (resid_drop): Dropout(p=0.1, inplace=False)
              (proj): Linear(in_features=72, out_features=72, bias=True)
            )
            (mlp): Sequential(
              (0): Linear(in_features=72, out_features=288, bias=True)
              (1): GELU()
              (2): Linear(in_features=288, out_features=72, bias=True)
              (3): Dropout(p=0.1, inplace=False)
            )
          )
          (2): Block(
            (ln1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
            (ln2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
            (attn): CausalSelfAttention(
              (key): Linear(in_features=72, out_features=72, bias=True)
              (query): Linear(in_features=72, out_features=72, bias=True)
              (value): Linear(in_features=72, out_features=72, bias=True)
              (attn_drop): Dropout(p=0.1, inplace=False)
              (resid_drop): Dropout(p=0.1, inplace=False)
              (proj): Linear(in_features=72, out_features=72, bias=True)
            )
            (mlp): Sequential(
              (0): Linear(in_features=72, out_features=288, bias=True)
              (1): GELU()
              (2): Linear(in_features=288, out_features=72, bias=True)
              (3): Dropout(p=0.1, inplace=False)
            )
          )
          (3): Block(
            (ln1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
            (ln2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
            (attn): CausalSelfAttention(
              (key): Linear(in_features=72, out_features=72, bias=True)
              (query): Linear(in_features=72, out_features=72, bias=True)
              (value): Linear(in_features=72, out_features=72, bias=True)
              (attn_drop): Dropout(p=0.1, inplace=False)
              (resid_drop): Dropout(p=0.1, inplace=False)
              (proj): Linear(in_features=72, out_features=72, bias=True)
            )
            (mlp): Sequential(
              (0): Linear(in_features=72, out_features=288, bias=True)
              (1): GELU()
              (2): Linear(in_features=288, out_features=72, bias=True)
              (3): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (ln_f): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
        (head): Linear(in_features=72, out_features=512, bias=False)
      )
    )
  )
)

SequenceDataset: loading dataset into memory...
  0%|          | 0/180 [00:00<?, ?it/s] 44%|####4     | 80/180 [00:00<00:00, 794.99it/s] 89%|########9 | 161/180 [00:00<00:00, 799.73it/s]100%|##########| 180/180 [00:00<00:00, 801.12it/s]
SequenceDataset: caching get_item calls...
  0%|          | 0/8640 [00:00<?, ?it/s] 15%|#5        | 1301/8640 [00:00<00:00, 13001.92it/s] 30%|###       | 2602/8640 [00:00<00:00, 12953.46it/s] 45%|####5     | 3898/8640 [00:00<00:00, 12928.78it/s] 60%|######    | 5215/8640 [00:00<00:00, 13019.81it/s] 76%|#######5  | 6539/8640 [00:00<00:00, 13098.00it/s] 91%|######### | 7849/8640 [00:00<00:00, 13069.70it/s]100%|##########| 8640/8640 [00:00<00:00, 13042.50it/s]
SequenceDataset: loading dataset into memory...
  0%|          | 0/20 [00:00<?, ?it/s]100%|##########| 20/20 [00:00<00:00, 852.10it/s]
SequenceDataset: caching get_item calls...
  0%|          | 0/1026 [00:00<?, ?it/s]100%|##########| 1026/1026 [00:00<00:00, 12932.74it/s]

============= Training Dataset =============
SequenceDataset (
	path=/home/andrew/Desktop/robo_sims/robomimic/datasets/lift/ph/low_dim.hdf5
	obs_keys=('object', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos')
	seq_length=10
	filter_key=train
	frame_stack=1
	pad_seq_length=True
	pad_frame_stack=True
	goal_mode=none
	cache_mode=all
	num_demos=180
	num_sequences=8640
)

  0%|          | 0/100 [00:00<?, ?it/s]  8%|8         | 8/100 [00:00<00:01, 70.46it/s] 16%|#6        | 16/100 [00:00<00:01, 71.63it/s] 24%|##4       | 24/100 [00:00<00:01, 71.15it/s] 32%|###2      | 32/100 [00:00<00:00, 70.85it/s] 40%|####      | 40/100 [00:00<00:00, 71.13it/s] 48%|####8     | 48/100 [00:00<00:00, 71.01it/s] 56%|#####6    | 56/100 [00:00<00:00, 71.15it/s] 64%|######4   | 64/100 [00:00<00:00, 71.48it/s] 72%|#######2  | 72/100 [00:01<00:00, 71.47it/s] 80%|########  | 80/100 [00:01<00:00, 71.89it/s] 88%|########8 | 88/100 [00:01<00:00, 71.90it/s] 96%|#########6| 96/100 [00:01<00:00, 71.90it/s]100%|##########| 100/100 [00:01<00:00, 71.50it/s]
Train Epoch 1
{
    "Cosine_Loss": 0.47063021957874296,
    "L1_Loss": 0.05991027358919382,
    "L2_Loss": 0.13027595154941082,
    "Loss": 3.2265916681289672,
    "Policy_Grad_Norms": 0.9999998728792368,
    "Time_Data_Loading": 0.0028720498085021973,
    "Time_Epoch": 0.02332245111465454,
    "Time_Log_Info": 7.024606068929036e-05,
    "Time_Process_Batch": 0.0004517555236816406,
    "Time_Train_Batch": 0.019877950350443523
}
  0%|          | 0/10 [00:00<?, ?it/s]100%|##########| 10/10 [00:00<00:00, 218.45it/s]
Validation Epoch 1
{
    "Cosine_Loss": 0.3339556992053986,
    "L1_Loss": 0.03349388595670462,
    "L2_Loss": 0.07290220893919468,
    "Loss": 2.657653737068176,
    "Time_Data_Loading": 0.0002773642539978027,
    "Time_Epoch": 0.0007801214853922526,
    "Time_Log_Info": 6.107489267985026e-06,
    "Time_Process_Batch": 4.234711329142253e-05,
    "Time_Train_Batch": 0.00043589274088541666
}

Epoch 1 Memory Usage: 3727 MB

  0%|          | 0/100 [00:00<?, ?it/s]  8%|8         | 8/100 [00:00<00:01, 71.50it/s] 16%|#6        | 16/100 [00:00<00:01, 71.99it/s] 24%|##4       | 24/100 [00:00<00:01, 71.91it/s] 32%|###2      | 32/100 [00:00<00:00, 71.63it/s] 40%|####      | 40/100 [00:00<00:00, 71.30it/s] 48%|####8     | 48/100 [00:00<00:00, 71.07it/s] 56%|#####6    | 56/100 [00:00<00:00, 70.66it/s] 64%|######4   | 64/100 [00:00<00:00, 71.01it/s] 72%|#######2  | 72/100 [00:01<00:00, 71.37it/s] 80%|########  | 80/100 [00:01<00:00, 71.48it/s] 88%|########8 | 88/100 [00:01<00:00, 71.68it/s] 96%|#########6| 96/100 [00:01<00:00, 71.40it/s]100%|##########| 100/100 [00:01<00:00, 71.35it/s]
Train Epoch 2
{
    "Cosine_Loss": 0.2682225042581558,
    "L1_Loss": 0.02783023154363036,
    "L2_Loss": 0.059289039075374604,
    "Loss": 2.2899538850784302,
    "Policy_Grad_Norms": 0.9999998985247851,
    "Time_Data_Loading": 0.0029140512148539224,
    "Time_Epoch": 0.023367687066396078,
    "Time_Log_Info": 7.0647398630778e-05,
    "Time_Process_Batch": 0.00045140981674194334,
    "Time_Train_Batch": 0.019882142543792725
}
  0%|          | 0/10 [00:00<?, ?it/s]100%|##########| 10/10 [00:00<00:00, 218.35it/s]
Validation Epoch 2
{
    "Cosine_Loss": 0.2524196535348892,
    "L1_Loss": 0.02794242911040783,
    "L2_Loss": 0.05930886566638947,
    "Loss": 2.151418447494507,
    "Time_Data_Loading": 0.00028198957443237305,
    "Time_Epoch": 0.0007806658744812012,
    "Time_Log_Info": 6.326039632161459e-06,
    "Time_Process_Batch": 4.144509633382162e-05,
    "Time_Train_Batch": 0.00043234427769978844
}

Epoch 2 Memory Usage: 3727 MB

